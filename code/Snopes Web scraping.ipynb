{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb684f9-c711-46eb-98be-e8ad91cbb9b4",
   "metadata": {},
   "source": [
    "## Plan\n",
    "Soo.. Snopes has a montly sitemap that contains all their articles and urls. My code pulls the 7 (Jan-July 2025) sitemap XMLs--- to get every URL for each month. Then, I need to visit each link only once and get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef71c422-9f7a-4dfc-8afe-6e042d24c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json, csv, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b362d4-daf1-4a79-a04e-051fcad25fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://media.snopes.com/sitemaps\"\n",
    "MONTHS = [f\"{m:02d}\" for m in range(1, 8)]  # Jan–Jul\n",
    "SITEMAPS = [f\"{BASE}/sitemap-articles-2025-{m}.xml\" for m in MONTHS]\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; DyuthiSnopesScraper/1.0; +https://example.org/)\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "OUT_CSV = Path(\"snopes_2025_JanJul.csv\") #Output as CSV--- my preferred output.\n",
    "OUT_JSONL = Path(\"snopes_2025_JanJul.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92279872-7d5d-493a-a7bf-e16bc89fb018",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14a1f729-1898-4cb7-b2f9-2047fcf23cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url, kind=\"html\", timeout=20):\n",
    "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.text if kind in (\"html\",\"xml\") else r.content\n",
    "\n",
    "def parse_sitemap(xml_text):\n",
    "    soup = BeautifulSoup(xml_text, \"xml\")\n",
    "    out = []\n",
    "    for url in soup.find_all(\"url\"):\n",
    "        loc = url.loc.get_text(strip=True)\n",
    "        lastmod = url.lastmod.get_text(strip=True) if url.lastmod else None\n",
    "        out.append({\"url\": loc, \"lastmod\": lastmod})\n",
    "    return out\n",
    "\n",
    "def extract_jsonld(soup):\n",
    "    # Grab the most complete JSON-LD block (Article/NewsArticle)\n",
    "    data = {}\n",
    "    for tag in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        try:\n",
    "            block = json.loads(tag.get_text(strip=True))\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        # Some pages wrap JSON-LD in a list\n",
    "        blocks = block if isinstance(block, list) else [block]\n",
    "        for b in blocks:\n",
    "            t = (b.get(\"@type\") or \"\").lower()\n",
    "            if \"article\" in t or \"newsarticle\" in t:\n",
    "                data = b\n",
    "                return data\n",
    "    return data\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt or \"\").strip()\n",
    "    return txt\n",
    "\n",
    "def extract_article(url):\n",
    "    html = fetch(url, \"html\")\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # Prefer JSON-LD for structured fields\n",
    "    ld = extract_jsonld(soup)\n",
    "\n",
    "    title = (ld.get(\"headline\") or\n",
    "             (soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else None))\n",
    "\n",
    "    author = None\n",
    "    if ld.get(\"author\"):\n",
    "        if isinstance(ld[\"author\"], list) and ld[\"author\"]:\n",
    "            author = ld[\"author\"][0].get(\"name\")\n",
    "        elif isinstance(ld[\"author\"], dict):\n",
    "            author = ld[\"author\"].get(\"name\")\n",
    "        elif isinstance(ld[\"author\"], str):\n",
    "            author = ld[\"author\"]\n",
    "\n",
    "    date_published = ld.get(\"datePublished\") or ld.get(\"dateCreated\") or None\n",
    "    if not date_published:\n",
    "        # Fallback: OpenGraph/article meta\n",
    "        meta = soup.find(\"meta\", attrs={\"property\": \"article:published_time\"})\n",
    "        if meta and meta.get(\"content\"):\n",
    "            date_published = meta[\"content\"]\n",
    "\n",
    "    # Body: prefer JSON-LD 'articleBody', else collect <article> paragraphs\n",
    "    body = ld.get(\"articleBody\")\n",
    "    if not body:\n",
    "        article_tag = soup.find(\"article\")\n",
    "        if article_tag:\n",
    "            paras = [p.get_text(\" \", strip=True) for p in article_tag.find_all([\"p\",\"li\"]) if p.get_text(strip=True)]\n",
    "            body = \"\\n\".join(paras) if paras else None\n",
    "\n",
    "    # Category (tag), rating (if fact-check)\n",
    "    category = None\n",
    "    rating = None\n",
    "    # Often present in breadcrumbs or JSON-LD\n",
    "    if ld.get(\"articleSection\"):\n",
    "        category = ld[\"articleSection\"] if isinstance(ld[\"articleSection\"], str) else \", \".join(ld[\"articleSection\"])\n",
    "\n",
    "    # Rating sometimes appears in page badges—heuristic scrape:\n",
    "    badge = soup.select_one('[class*=\"rating\"], [class*=\"badge\"]')\n",
    "    if badge:\n",
    "        rating = badge.get_text(\" \", strip=True)\n",
    "\n",
    "    # Normalize date\n",
    "    pub_dt = None\n",
    "    if date_published:\n",
    "        try:\n",
    "            pub_dt = dateparser.parse(date_published)\n",
    "        except Exception:\n",
    "            pub_dt = None\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"title\": clean_text(title),\n",
    "        \"author\": clean_text(author),\n",
    "        \"date_published\": pub_dt.isoformat() if pub_dt else date_published,\n",
    "        \"category\": clean_text(category),\n",
    "        \"rating\": clean_text(rating),\n",
    "        \"body\": body.strip() if body else None,\n",
    "    }\n",
    "\n",
    "def in_range_2025_jan_jul(dt_iso):\n",
    "    if not dt_iso:\n",
    "        return True  # keep if unknown (sitemaps already month-scoped)\n",
    "    try:\n",
    "        d = dateparser.parse(dt_iso)\n",
    "        return datetime(2025,1,1) <= d <= datetime(2025,7,31,23,59,59)\n",
    "    except Exception:\n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b065803-8178-4f10-97c2-4d14cdda58aa",
   "metadata": {},
   "source": [
    "## Run Fns- Let's scrapee!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be8c2d-a28c-4fab-8946-7fd1f2bd7884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sitemaps…\n",
      "Found 1809 article URLs from Jan–Jul 2025 sitemaps.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping:  99%|███████████████████████████████████████████████████████████████████▌| 1797/1809 [31:10<00:11,  1.02it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # 1) Collect URLs from monthly sitemaps\n",
    "    print(\"Fetching sitemaps…\")\n",
    "    urls = []\n",
    "    for sm in SITEMAPS:\n",
    "        try:\n",
    "            xml = fetch(sm, \"xml\")\n",
    "            urls += parse_sitemap(xml)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed sitemap {sm}: {e}\")\n",
    "\n",
    "    # Deduplicate while keeping latest modification of article\n",
    "    seen = {}\n",
    "    for u in urls:\n",
    "        seen[u[\"url\"]] = u.get(\"lastmod\")\n",
    "    url_list = list(seen.keys())\n",
    "    print(f\"Found {len(url_list)} article URLs from Jan–Jul 2025 sitemaps.\")\n",
    "\n",
    "    # 2) Visit links & extract\n",
    "    rows = []\n",
    "    with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as jf:\n",
    "        for url in tqdm(url_list, desc=\"Scraping\"):\n",
    "            try:\n",
    "                art = extract_article(url)\n",
    "                if in_range_2025_jan_jul(art[\"date_published\"]):\n",
    "                    rows.append(art)\n",
    "                    jf.write(json.dumps(art, ensure_ascii=False) + \"\\n\")\n",
    "            except Exception as e:\n",
    "                rows.append({\"url\": url, \"error\": str(e)})\n",
    "            time.sleep(0.8)  # slowwwwwwwwly to respect site rates\n",
    "\n",
    " # 3) Save CSV\n",
    "    fieldnames = [\"url\",\"title\",\"author\",\"date_published\",\"category\",\"rating\",\"body\"]\n",
    "    with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k, \"\") for k in fieldnames})\n",
    "\n",
    "    print(f\"Wrote {len(rows)} records to {OUT_CSV} and {OUT_JSONL}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ce3d8-b50d-4d95-bff3-e3f518ae2de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
